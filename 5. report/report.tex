\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\setlength{\columnsep}{10mm}
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{textcomp}
\usepackage[dvipsnames]{xcolor}
\usepackage{hyperref}
\usepackage{float}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Outperforming Morningstar Analysts: \\ Applying Enhanced LLMs to Financial Reports
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and should not be used}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Jonas Gottal }
\IEEEauthorblockA{\textit{LMU Munich} \\
\textit{Department of Computer Science}\\
%City, Country \\
%email address or ORCID
}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Mohamad Hgog}
\IEEEauthorblockA{\textit{LMU Munich} \\
\textit{Department of Computer Science}\\
%City, Country \\
%email address or ORCID
}
}

\maketitle

\begin{abstract}
This study examines financial analyst reports from Morningstar, revealing two key findings: firstly, analysts appear to select stocks arbitrarily, and secondly, they provide comprehensive textual justifications that enable informed decisions surpassing mere chance. We employ enhanced large language models to efficiently analyze the textual data, facilitating a broad exploration of various pre-trained models to identify the subtle underlying sentiment and extract more value from the reports than the experts themselves. The code is available at \href{https://github.com/trashpanda-ai/Advanced-Analytics-and-Machine-Learning}{github.com/trashpanda-ai}. %Describe Goal, data, method and results.
\end{abstract}

\begin{IEEEkeywords}
LLMs, PEFT, Adapters, Hugging Face, Finance
\end{IEEEkeywords}

\section{Introduction}
Many search for guidance in financial analyst reports for investment purpose. Especially during the COVID-19 pandemic, the number of retail traders has increased significantly. And Morningstar is one of the most popular platforms for financial reports that are widely used by both retail traders and professionals.

However, the question arises whether these reports are actually valuable or just an illusion of expertise. In this paper, we investigate the value of analyst reports and whether they can outperform the market. We hypothesize that the reports contain valuable information that can be extracted using enhanced large language models (LLMs) to outperform the analysts and even the market.
\section{Data} %Jonas %Report and financial market data.
Financial analysts provide detailed reports on single companies (equities), detailing their current situation and future prospects. These reports are based on a variety of factors, including their financial strength, economic moat and cost allocation. The reports also contain a rating and a fair price estimate. 

\subsection{Data description}%Jonas
The data is retrieved from \href{https://www.morningstar.co.uk/uk/}{Morningstar} and contains the following columns:
\begin{itemize}    
    \item \texttt{ParseDate}: The date the information was retrieved.
  \item \texttt{Title}: The title of the analyst report.
  \item \texttt{CompanyName}: The name of the company. 
  \item \texttt{TickerSymbol}: The ticker symbol (without exchange information) of the underlying stock.
  \item \texttt{Rating}: The analyst rating of the stock.
  \item \texttt{ReportDate}: The date of the release of the report. 
  \item \texttt{AuthorName}: The name of the author of the report. 
  \item \texttt{Price}: The price of the stock declared in the report.
  \item \texttt{Currency}: The given currency of the price. 
  \item \texttt{PriceDate}: The date the price was retrieved from market data (Morningstar).
  \item \texttt{FairPrice}: The estimated fair price from the analyst.
  \item \texttt{Uncertainty}: The company's uncertainty quantified in 'Low', 'Medium', 'High' and 'Very High'.
  \item \texttt{EconomicMoat}: The ability to maintain competitive advantages quantified in 'Narrow' and 'Wide'.
  \item \texttt{CostAllocation}:  Decisions on investments categorized as 'Poor', 'Standard', and 'Exemplary'.
  \item \texttt{FinancialStrength}: Rating of the ability to make timely payments and fulfill obligations -- quantified in 'A', 'B', ... 'F'.
  \item \texttt{AnalystNoteDate}: The date of the analyst note.
  \item \texttt{AnalystNoteList}: The analyst note.
  \item \texttt{BullsList}: Arguments in favor of the company.
  \item \texttt{BearsList}: Arguments against the company.
  \item \texttt{ResearchThesisDate}: The date of the thesis.
  \item \texttt{ResearchThesisList}: Objective research thesis.
  \item \texttt{MoatAnalysis}: Added research on EconomicMoat.
  \item \texttt{RiskAnalysis}: Company's risk profile. 
  \item \texttt{CapitalAllocation}: Text on the CostAllocation. 
  \item \texttt{Profile}: Short text on the company profile.
  \item \texttt{FinancialStrengthText}: Short text on the financial strength of the company (conclusion).
\end{itemize}
The most important information for the application of LLMs is the textual data -- starting with \texttt{AnalystNoteList}, which we expect to have the most effective content due to its subjective tone. We also analyze more objective texts like \texttt{BullsList} and \texttt{BearsList} as well as \texttt{ResearchThesisList}, \texttt{MoatAnalysis}, \texttt{RiskAnalysis}, \texttt{CapitalAllocation}, \texttt{Profile} and \texttt{FinancialStrengthText}.
Since the target group of financial analyst reports is investors, we focus on the long term perspective and not on short-term speculation. Therefore, we look at the stock price development over a longer period. The data is retrieved from Yahoo Finance and contains the following columns: \texttt{Date, Open, High, Low, Close, Adjusted Close} and \texttt{Volume}. We base our target variable for the sentiment analysis on the \texttt{Adjusted Close} price.

\subsection{Data Preprocessing}%Jonas
Since we care about investing and not speculation, we pre-process the data to remove spurious spikes and other noise. We focus on the archetype of a good investment as shown in Figure~\ref{fig:preprocessing1} and ignore spurious spikes as shown in Figure~\ref{fig:preprocessing2} and Figure~\ref{fig:preprocessing3}. This means we consider the average performance within a certain time period -- here the maximum amount since the last report was released: 120 days. Inspired by the commonly used moving averages in mathematical finance \cite{Hull2021}, the preprocessing is done by calculating the average stock price over 120 days since publication and comparing it to the stock price at the time of the report. If the average stock price is higher\footnote{A sideway movement within a 2\% margin is not classified as ``higher''.} than the price at time of release, we label it as a good investment, otherwise as a bad investment. This is our binary target variable for the following analysis.

\begin{figure}[h!]
    \centering
    \begin{subfigure}{.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../5. report/pictures/preproccessing1.png}
        \caption{Archetype}
        \label{fig:preprocessing1}
    \end{subfigure}%
    \begin{subfigure}{.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../5. report/pictures/preproccessing2.png}
        \caption{Pure luck}
        \label{fig:preprocessing2}
    \end{subfigure}%
    \begin{subfigure}{.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../5. report/pictures/preproccessing3.png}
        \caption{Bad luck}
        \label{fig:preprocessing3}
    \end{subfigure}
    \caption{Different stock developments as charts: Comparing the archetype of a good investment to spurious spikes (positive and negative). }
    \label{fig:preprocessing}
\end{figure}

The final data (1586 reports) is split into a training, development and test set with a ratio of 70:10:20 (1103, 165, 318). The training set is used to train the models, the development set to optimize the hyperparameters and the test set to evaluate the models. 

\section{Foundations} %Mohamad
Our research efficiently leverages the capabilities of LLMs by utilizing Transformers and Adapters, primarily sourced from Hugging Face \cite{wolf2020huggingfaces}. This approach is fundamentally driven by the need to process extensive textual data accurately and efficiently, particularly focusing on sentiment classification within financial texts.

Utilizing LLMs, including those based on Transformer architectures, has demonstrated unparalleled success in understanding and generating human-like text. Their ability to capture deep linguistic and semantic nuances makes them ideal for complex NLP tasks like sentiment analysis. However, the computational intensity of training such models from scratch demands significant resources \cite{wolf2020huggingfaces}.

To efficiently handle LLMs, we use the efficient Transformer architecture and models provided by Hugging Face \cite{wolf2020huggingfaces}. We also implement Adapters, which offer a parameter-efficient way to fine-tune pre-trained models on domain-specific tasks \cite{pfeiffer2020adapterhub}. By inserting small, trainable modules within the Transformer layers, we can tailor the model's responses to the subtleties of financial language without overhauling the entire network architecture. This approach significantly reduces the computational overhead and facilitates quicker iterations and refinements.

Our research aims to classify the sentiment expressed in Analyst Reports from MorningStar accurately. Analyzing the sentiment, we can extract meaningful insights about market trends and investor perceptions, which are crucial for making informed financial decisions. Sentiment classification in this context involves determining the polarity of the text—whether it conveys a positive, negative, or neutral sentiment—which can significantly influence investment strategies and business outcomes.

\subsection{Transformer models}%Mohamad
Transformer models are a class of deep learning architectures that have revolutionized the field of natural language processing (NLP). Based on the encoder-decoder architecture, these models leverage self-attention mechanisms to process data sequences in parallel and capture complex dependencies across the sequence \cite{Vaswani2023} \cite{wolf2020huggingfaces}. The encoder maps an input sequence of symbol representations $(x_1, \ldots, x_n)$ to a continuous representation $\mathbf{z}=(z_1, \ldots, z_n)$. The decoder generates an output sequence $(y_1, \ldots, y_m)$ from $\mathbf{z}$, using an inherently auto-regressive method, processing previous symbols to generate the next \cite{Vaswani2023}.

Figure \ref{fig:transformer_architecture} provides a detailed view of the Transformer architecture, illustrating both the encoder and decoder components.

The backbone of the Transformer is the self-attention mechanism in both the encoder and decoder stacks, which allows each position in the sequence to attend to all positions in the previous layer of the model. Each encoder and decoder layer consists of multi-head self-attention mechanisms followed by position-wise fully connected feed-forward networks. Residual connections and layer normalization are employed around each sub-layer to facilitate the training of deep networks \cite{Vaswani2023}. Formally, each layer output can be described as:
\[
\text{Layer Output} = \text{LayerNorm}(x + \operatorname{Sublayer}(x)),
\]
where $\operatorname{Sublayer}(x)$ represents the operations within the sub-layer itself.

\textbf{Self-attention} allows the model to weigh the importance of different words in the sequence irrespective of their positional distances. The attention function can be described as mapping a query and a set of key-value pairs to an output, where the output is a weighted sum of the values. A compatibility function of the query with the corresponding key computes the weight assigned to each value. The scaled dot-product attention is computed as:
\[
\operatorname{Attention}(Q, K, V) = \operatorname{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V,
\]
where $Q$, $K$, and $V$ are the matrices representing queries, keys, and values, respectively, and $d_k$ is the dimensionality of the keys.

\textbf{Multi-head attention}, a pivotal extension of the basic attention mechanism, projects the queries, keys, and values multiple times with different, learned linear transformations to allow the model to attend to information from different representation subspaces jointly:
\[
\begin{aligned}
\operatorname{MultiHead}(Q, K, V) &= \operatorname{Concat}(\operatorname{head}_1, \ldots, \operatorname{head}_h) W^O \\
\text{where head} &= \operatorname{Attention}(QW_i^Q, KW_i^K, VW_i^V).
\end{aligned}
\]
Here, $W_i^Q$, $W_i^K$, and $W_i^V$ are parameter matrices for different heads, and $W^O$ is the output projection matrix.

Furthermore, the model incorporates \textbf{position-wise feed-forward networks} in each layer, consisting of two linear transformations with a ReLU \cite{agarap2019deep} activation in between:
\[
\operatorname{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
\]

Positional encodings are added to input embeddings to give the model access to the position of the tokens in the sequence. This is crucial since the model lacks recurrence or convolution:
\[
\begin{aligned}
PE_{(pos, 2i)} &= \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) \\
PE_{(pos, 2i+1)} &= \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right),
\end{aligned}
\]
where $pos$ is the position and $i$ is the dimension.

\begin{figure}[!h]
    \centering
    \includegraphics[width=.85\linewidth]{pictures/transformer_architecture.png}
    \caption{The architecture of a Transformer model, showing the flow from input through the encoder and decoder to the output \cite{Vaswani2023}.}
    \label{fig:transformer_architecture}
\end{figure}

\subsection{Parameter efficient fine-tuning (PEFT)}%Mohamad
When fine-tuning large pre-trained models for various tasks, computational efficiency and scalability require innovative approaches. Parameter-efficient fine-tuning (PEFT) addresses this challenge by enabling the adaptation of large models to specific tasks without the need to update all model parameters. Thus, PEFT conserves computational resources and enhances the feasibility of exploring multiple models \cite{pfeiffer2020adapterhub}.

\textbf{Adapters} are central to the PEFT strategy. An adapter is a small neural network module inserted between the layers of a pre-existing model, such as a Transformer. As illustrated in Figure \ref{fig:PEFT1}, the adapter modifies the Transformer block by inserting additional neural network layers that allow for fine-tuning on specific tasks with minimal updates to the overall model parameters.

Typically, these adapters consist of a down-projection that reduces the dimensionality of the layer output, a non-linear activation function, and an up-projection that restores the dimensionality to its original size \cite{pfeiffer2020adapterhub}. The formal structure of an adapter, shown in Figure \ref{fig:PEFT2}, can be expressed as:
\[
\operatorname{Adapter}(x) = x + U(\sigma(D(x))),
\]
where $x$ is the input to the adapter, $D$ represents the down-projection, $\sigma$ is a non-linear activation function (e.g., ReLU), and $U$ is the up-projection.

Adapters offer a significant advantage in that only the parameters within these modules need to be updated during fine-tuning, leaving the vast majority of the pre-trained model's parameters frozen. This is particularly advantageous for several reasons:
\begin{itemize}
    \item \textbf{Computational Efficiency:} Since only a small fraction of the overall parameters are updated, the computational overhead is significantly reduced compared to full model fine-tuning.
    \item \textbf{Preservation of Pre-trained Knowledge:} By keeping most of the original model parameters unchanged, the risk of catastrophic forgetting is minimized, thus preserving the knowledge that the model has acquired during its initial extensive training.
    \item \textbf{Scalability:} Adapters allow for quick adaptation to multiple tasks without the need for extensive retraining, making it feasible to fine-tune large models on a wide range of tasks, even with limited computational resources.
\end{itemize}

\cite{Poth2023}

\begin{figure}[h!]
    \centering
    \includegraphics[width=.65\linewidth]{pictures/PEFT1.png}
    \caption[PEFT1]{Process of PEFT \cite{Raschka2023}.}
    \label{fig:PEFT1}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=.65\linewidth]{pictures/PEFT2.jpg}
    \caption[PEFT2]{Process of PEFT (detailed) \cite{Raschka2023}.}
    \label{fig:PEFT2}
\end{figure}




\subsection{Hugging Face}%Mohamad
Hugging Face is an essential platform in NLP and machine learning. It provides an extensive repository of pre-trained models and tools for training, fine-tuning, and deploying models across various tasks \cite{wolf2020huggingfaces}. The platform is renowned for implementing state-of-the-art models based on Transformer architectures like BERT \cite{devlin2019bert}, GPT \cite{brown2020language}, and their derivatives.

\textbf{Why Use Hugging Face?} The primary appeal of Hugging Face lies in its comprehensive ecosystem that simplifies the deployment of machine learning models. There are several reasons why it is widely used:
\begin{itemize}
    \item \textbf{Accessibility of Pre-trained Models:} Hugging Face offers access to thousands of pre-trained models that are easily adaptable to a broad range of tasks without the need for scratch training.
    \item \textbf{Ease of Integration:} The platform supports various programming languages and frameworks, including Python, PyTorch \cite{paszke2019pytorch}, and TensorFlow \cite{tensorflow2015-whitepaper}, enabling seamless integration into existing projects.
    \item \textbf{Community and Support:} With a robust community of developers and researchers, Hugging Face facilitates collaboration and sharing of best practices, contributing to the continual improvement and extension of its model offerings.
\end{itemize}

\textbf{Benefits in Practical Applications:} Utilizing Hugging Face allows us to reduce the time and resources required for model development dramatically. Specifically, the platform provides functionalities for:
\begin{itemize}
    \item \textbf{Fine-Tuning Pre-trained Models:} Users can fine-tune models on specific datasets, which is essential for tailoring the model's responses to the nuances of particular domains or tasks.
    \item \textbf{Experimentation:} The ease of accessing and modifying different models promotes rapid prototyping and experimentation, a valuable feature in the fast-evolving field of AI.
    \item \textbf{Deployment:} Hugging Face also offers solutions for easy deployment of trained models, which is beneficial for bringing AI applications to production.
\end{itemize}

\textbf{Application in Our Work:} In our project, we leverage Hugging Face to fine-tune pre-trained models specifically for analyzing Analyst Reports from MorningStar. This involves adapting state-of-the-art Transformer models \cite{wolf2020huggingfaces} to comprehend and interpret complex financial texts, which is critical for extracting actionable insights and automating parts of the financial analysis process. The ability to fine-tune and deploy models efficiently with Hugging Face accelerates our research and application development, enabling a focused approach to model performance and application-specific adjustments rather than infrastructure and model maintenance.

\section{literature}%Jonas

As described in \cite{Zhao2024}, there are many different applications of LLMs, and specifically for analyst reports they can provide insights into subtle tone and sentiment to add value. However, at the moment there is only research based on   data created by consumers, such as Reddit \cite{Deng2023}. Furthermore, it has been shown that reports may have a small positive performance in the right market conditions \cite{Su2020}, but it is unclear whether only recommendations themselves can influence markets \cite{Brauer2018} and reports have no value at all \cite{Panchenko2007}. Thus, the herding factor may have an impact on stock performance \cite{Palmer2018}.
In \cite{Kim2023} they use LLMs to better interpret and analyze Korean financial analysts' reports.
In this paper, however, we use data from widely spread reports (from retail traders to professionals) on globally traded firms. We show that while analysts can barely outperform the current market, they provide comprehensive textual justifications that enable informed decisions. We use enhanced large language models to efficiently analyze the textual data, allowing a broad exploration of different pre-trained models to identify the subtle underlying sentiment and extract more value from the reports than the experts themselves.


\section{Approach}%Jonas
Our ultimate objective can be interpreted as a binary classification problem based on sentiment analysis. We use the textual data from the reports to predict whether the stock is a good investment or not. In order to do so we compare the performance of different models and configurations to find the best one. We also compare the performance of the models to the analysts' own predictions and other metrics from the report to create a benchmark. In order to explore many different models and configurations, we use the parameter efficient fine-tuning approach. We use adapters to fine-tune the models and Hugging Face to access the pre-trained models. Even though literature has shown adapters to be more efficient while retaining the performance of the underlying model, we setup a preliminary experiment with the Stanford Sentiment Treebank v2 (SST2) data set for sentiment classification to validate our approach and compared the efficiency and performance of the models:
% here a table with results for

\begin{table}[h!]
    \centering
    \begin{tabular}{lcc}
    \toprule
     & \textbf{Fine-tuned LLM} & \textbf{Fine-tuned Adapter} \\
     \midrule
    {Training Runtime} & 1h 58m & \textbf{\textcolor{ForestGreen}{24m}} \\
    {Evaluating Accuracy} & \textbf{\textcolor{ForestGreen}{0.908}} & 0.902 \\
    {Evaluation Runtime} & 6.83s & \textbf{\textcolor{ForestGreen}{1.84s}} \\
    {Evaluation Loss} & 0.42 & \textbf{\textcolor{ForestGreen}{0.31}} \\
    \bottomrule
    \end{tabular}
    \caption{Comparison of Fine-tuned LLM and Fine-tuned Adapter}
    \label{tab:comparison}
    \end{table}


Therefore, we used PEFT to find the best performing setup and built a automated training and evaluation pipeline to efficiently explore the model landscape of Hugging Face.

\subsection{Model selection}%Jonas
Since our approach is centered around sentiment classification of financial reports, we allocated the most fitting pre-trained models from both worlds -- sentiment analysis and finance:


\noindent
\textbf{Sentiment:}
\begin{itemize}
\item \texttt{kwang123/bert-sentiment-analysis}
\item \texttt{siebert/sentiment-roberta- large-english}
\item \texttt{distilbert/distilbert-base- uncased-finetuned-sst-2-english}
\end{itemize}

\noindent
\textbf{Finance:}
\begin{itemize}
\item \texttt{ProsusAI/finbert}
\item \texttt{yiyanghkust/finbert-tone}
\item \texttt{bardsai/finance-sentiment-pl-fast}
\item \texttt{RashidNLP/Finance-Sentiment- Classification}
\item \texttt{ahmedrachid/FinancialBERT- Sentiment-Analysis}
\item \texttt{soleimanian/financial- roberta-large-sentiment}
\item \texttt{nickmuchi/sec-bert-finetuned- finance-classification}
\item \texttt{nickmuchi/deberta-v3-base- finetuned-finance-text-classification}
\end{itemize}

Within the financial subset, there are also models fine-tuned to the task of sentiment classification on financial text data, such as publicly available SEC filings.

\subsection{Adapter configuration}%Mohamad
Configuring adapters for machine learning models, particularly in Transformer architectures, allows for targeted modifications to model behavior while preserving the bulk of the pre-trained parameters. Adapters offer a flexible approach to fine-tuning, where specific components are added or adjusted to alter the model's functionality for specialized tasks \cite{pfeiffer2020adapterhub}.

\textbf{Adapter Configuration Options:} The configuration of adapters can vary widely depending on the desired outcome and the specific model architecture. Common configurations include altering the number of adapter layers, the size of the feed-forward networks, the non-linearity type, and the training strategy. The choice of configuration typically balances between computational efficiency and task-specific performance \cite{min2023recent}.

\textbf{Industry Standard and Our Usage:} For the industry, especially in areas involving complex sequence understanding like financial analysis, configurations that offer a good trade-off between performance and computational demand are preferred. In our work, we experimented with several adapter configurations to identify the most effective setup for analyzing financial reports.

\textbf{Adapter Configurations Used in Our Work:}
\begin{itemize}
    \item \textbf{LoRAConfig:} LoRA (Low-Rank Adaptation) employs rank decomposition to modify attention and feed-forward layers within a Transformer. By adjusting only a small subset of model parameters, LoRAConfig focuses on the adaptability of the model with minimal computational overhead \cite{hu2021lora}.
    \item \textbf{UniPELTConfig:} The Unified Framework for Parameter-Efficient Language Model Tuning (UniPELT) is a framework that combines various parameter-efficient techniques as sub-modules. UniPELTConfig eliminates the need for model selection by optimizing sub-module activation based on their contribution to the task, ensuring consistently high performance while maintaining model efficiency \cite{mao2022unipelt}.
    \item \textbf{MAMConfig:} The Mix-And-Match (MAM) adapter optimizes attention sub-layers efficiently. It adapts to new tasks quickly with fewer parameters updated and achieves comparable results to full fine-tuning. MAM adapter has strong performance in adjusting to new data distributions and task requirements, making it an optimal choice for rapid deployment scenarios \cite{he2022unified}.
    \item \textbf{ConfigUnion (LoRAConfig, PrefixTuningConfig, SeqBnConfig):} This configuration combines LoRA, Prefix Tuning \cite{li2021prefixtuning}, and Sequence Bucketing strategies \cite{pfeiffer2020madx}. Prefix Tuning involves prepending a sequence of trainable vectors to the input, while Sequence Bucketing adjusts the model based on sequence length. This union aims to leverage the strengths of each individual approach for enhanced overall performance \cite{pfeiffer2020adapterhub}.
\end{itemize}

\subsection{Exploration and Exploitation}%Jonas
Since running all possible combinations of configurations is intractable, we assume independence of our various parameters in our experimental setup and base our approach on exploration and exploitation. We explore all possible valid models and configurations and exploit the best ones in the next iteration. First, we compare the different models, then we optimize for the best adapter configuration, and after that we search for the best text input. Finally, we compare our PEFT approach to fine-tuning the LLM itself.
All experiments are performed on the same data, using the same training and test split to ensure comparability, and we use the same evaluation metrics, training parameters and hyperparameters for all models. Everything is run on the same hardware.

\subsection{Evaluation} %Jonas
In order to validate learned models we compare predictions made from the model with previously separated test data. In this way, we can objectively evaluate them according to their predictive qualities. For this comparison, we utilize these fundamental principles:
\newline \textit{Sensitivity} or true positive rate ($\mathrm{TPR}$) is derived from the true positives $\mathrm{TP}$,  i.e., the correctly identified positives $\mathrm{P}$ from the test set:
$
\mathrm{TPR}=\frac{\mathrm{TP}}{\mathrm{P}}
$
\newline \textit{Specificity} or true negative rate ($\mathrm{TNR}$) is derived from the true negatives $\mathrm{TN}$,  i.e., the correctly identified negatives $\mathrm{N}$ from the test set:
$
\mathrm{TNR}=\frac{\mathrm{TN}}{\mathrm{N}}
$

\begin{figure}[h!]
    \centering
    \includegraphics[width=.85\linewidth]{pictures/ROC.pdf}
    \caption[ROC explanation]{Exemplary receiver operating curves for  a \textit{good, medium, random} and \textit{inverse} model fit. The perfect score would be 1.0 on each dimension -- specificity and sensitivity.}
    \label{fig:exroc}
    \end{figure}

By plotting both the sensitivity and specificity in relation for various threshold values, we obtain the so called Receiver Operating Characteristic (ROC) curve. For the results of both measures 1.0 is the optimum, and if the curve is the diagonal, we observed a random process.  In Figure~\ref{fig:exroc}, we can see some common examples of curves. In order to further summarize these evaluations, we can calculate the Area Under the Curve (AUC) to rank the models. Again, a value of 0.5 indicates a random process.
The reason we use ROC rather than accuracy, precision, recall and f1 scores is that ROC is a more comprehensive metric that provides a more detailed view of model performance. In addition, ROC curves make it easier to compare the performance of different models across all classification thresholds. This also makes it more robust to class imbalance, which is very important for inherently unbalanced data such as financial markets, which often follow trends and alternate between bear and bull markets. It also helps to understand how the performance of the model changes with different classification thresholds, which can be crucial for decision making in different conditions where the cost of false positives and false negatives can vary. \cite{Hastie2009}  \cite{Kauermann2021}  \cite{Russell2021}



\section{Results}%Jonas
In this section we present the results of our experiments. We compare the models with classical approaches based on the analysts' forecasts themselves and other metrics from the report to establish a benchmark. We present the results of our model training in three steps: First, we compare the different models, then we optimize the best adapter configuration, and finally we search for the text input that provides the highest predictive power. We also directly fine-tune the underlying model to see if PEFT loses significant predictive power. Furthermore, we use feature attribution via CAPTUM to gain insight into the models and provide an overview of training time, accuracy and number of parameters.

\subsection{Benchmarking}%Jonas
In order to evaluate our models accurately, we first create a benchmark based on the analysts' own predictions. We use the analysts' \texttt{FairPrice} to create binary predictions and use them  as input for a logistic regression model as a first benchmark. We also use all the categorical (and ordinal) information from the reports as input for an gradient boosted tree model (XGBoost \cite{Russell2021}) as a benchmark: \texttt{Rating}, \texttt{AuthorName}, \texttt{Uncertainty}, \texttt{EconomicMoat}, \texttt{CostAllocation}, \texttt{FinancialStrength}, \texttt{Sector}, \texttt{Exchange}, \texttt{Prediction}. We assume that certain authors might be better than others, and that the uncertainty, economic moat, cost allocation and financial strength might be valid indicators of a firms success. We also assume that the sector and exchange might have an impact on the stock performance. We use the same training and test split as for the LLMs and the same evaluation metrics. The results are shown in Figure~\ref{fig:Zeroexp}.

\begin{figure}[H]
    \centering
    \includegraphics[width=.85\linewidth]{../3. evaluation/roc_curves/Zero Experiment.png}
    \caption[First Experiment]{Benchmark Experiment: a diagonal line represents a random process. For financial markets it means reproducing the current distribution (following the trend).}
    \label{fig:Zeroexp}
\end{figure}

The results show that neither the analysts nor the XGBoost model can outperform randomness -- it only reproduces the current distribution of the market (trend following). This is a strong indication that the reports themselves may not contain any valuable information.


\subsection{Models in comparison}%Jonas
Our approach assumes that all changes in our experiments are independent so that the results are comparable. As first step we compare the different models on the same adapter configurations and text inputs to find the best one.

\begin{figure}[H]
    \centering
    \includegraphics[width=.85\linewidth]{../3. evaluation/roc_curves/First Experiment.png}
    \caption[First Experiment]{First Experiment}
    \label{fig:Firstexp}
\end{figure}

We continue for the two best models (both trained on financial sentiment data) and compare the best adapter configurations found in literature:

\begin{figure}[H]
    \centering
    \includegraphics[width=.85\linewidth]{../3. evaluation/roc_curves/Second Experiment.png}
    \caption[Second Experiment]{Second Experiment}
    \label{fig:Secondexp}
\end{figure}

Next to \texttt{AnalystNoteList} we also explore  \texttt{CapitalAllocation}, \texttt{BullsList}, \texttt{BearsList}, \texttt{MoatAnalysis}, \texttt{ResearchThesisList},  \texttt{Risk Analysis}, \texttt{Profile}, and  \texttt{FinancialStrengthText} as input data. We find that \texttt{FinancialStrengthText} as a concise summary provides the best predictive power.

\begin{figure}[H]
    \centering
    \includegraphics[width=.85\linewidth]{../3. evaluation/roc_curves/Third Experiment.png}
    \caption[Third Experiment]{Third Experiment}
    \label{fig:Thirdexp}
\end{figure}

Finally, we use the best combination found to directly fine tune the underlying model. This leads to only a small improvement of 0.0048 AUC and the training time was slightly higher (from ~480 sec to 520 sec), but the number of parameters could be drastically decreased from 110 million to 23 million.

\begin{figure}[H]
    \centering
    \includegraphics[width=.85\linewidth]{../3. evaluation/roc_curves/Fourth Experiment.png}
    \caption[Fourth Experiment]{Final Result.}
    \label{fig:Fourthexp}
\end{figure}

While there is no overall cutoff value and it highly depends on the specific application, with values above 0.7 \cite{Hosmer2013}, we reached a robust predictive quality.

\subsection{Insights via CAPTUM}%Mohamad
Captum, derived from the Latin word for "understanding," is an interpretability library developed by Meta AI for the PyTorch framework \cite{paszke2019pytorch}. It features several algorithms to identify data feature contributions. It helps validate model behavior, ensure fairness, and debug complex machine learning applications \cite{Kokhlikyan2020}.

A key method in Captum for feature attribution is the \textit{Layer Integrated Gradients}, which extends the Integrated Gradients \cite{sundararajan2017axiomatic} approach. This method is particularly adept at handling models with deep architectures, such as BERT \cite{devlin2019bert}, by attributing importance to input features relative to a specific layer's output, thus offering more granular insight \cite{sundararajan2017axiomatic}.

In our project, we utilize Captum to analyze and validate the feature attributions of our fine-tuned models on financial texts from MorningStar analyst reports. This ensures that our models' predictive decisions are based on relevant and justifiable data, fostering trust and transparency in automated financial analysis.

The first case (see \ref{fig:high1}) showcases an input where the model's prediction of financial strength was substantially higher. This text highlights a high financial position characterized by a high level of cash reserves, no debt, and a positive free cash flow, indicating strong financial health and operational efficiency.

In contrast, the second plot (see \ref{fig:low3}) corresponds to an input scenario where the model's prediction of financial strength was at its lowest. The text discusses significant concerns about a company's ability to meet near-term obligations due to a notable increase in its debt-equity ratio and a sharp decline in interest coverage ratios.

These contrasting scenarios underscore the effectiveness of Captum in providing meaningful insights into how specific features in the text influence the model's output, thus assisting in understanding the underlying reasons behind model predictions.


\begin{figure}[!h]
    \centering
    \includegraphics[width=.85\linewidth]{pictures/high_1.png}
    \caption{Text input from Analyst Report indicating high financial strength prediction.}
    \label{fig:high1}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=.85\linewidth]{pictures/low_3.png}
    \caption{Text input from Analyst Report indicating low financial strength prediction.}
    \label{fig:low3}
\end{figure}



\subsection{Training analysis}%Mohamad
Training analysis involves systematically evaluating model performance metrics across different configurations to optimize accuracy, computational efficiency, and effectiveness. In our work, we analyze the impact of various Adapter configurations on Transformer-based models dedicated to sentiment classification tasks within the financial domain.

\textbf{Number of Parameters vs. Accuracy:} The first plot \ref{fig:param_acc} in our analysis illustrates the relationship between the number of parameters in different Adapter configurations and the achieved accuracy. The configurations exhibit diverse accuracies; notably, MAMConfig, which possesses a higher parameter count, tends to achieve greater accuracy. On the other hand, configurations like LoRAConfig and UniPELTConfig show a broad spread in results, highlighting their varying effectiveness dependent on specific implementation and training data nuances.

\textbf{Training Time vs. Accuracy:} The second plot \ref{fig:time_acc} correlates the training time with accuracy, shedding light on the computational efficiency of each Adapter configuration. Here, MAMConfig configurations demonstrate high accuracy but require longer training times, which may not be feasible in scenarios necessitating rapid model deployment. Conversely, ConfigUnion presents a compromise, offering a balanced trade-off between training duration and accuracy, suggesting its potential suitability for practical applications.

\textbf{Model Accuracy with LoRA Adapter Configuration:} The third plot \ref{fig:models_lora_acc} displays the accuracies achieved by different models using the LoRA Adapter configuration. While models such as "yiyanghkust/finbert-tone" and "nickmuchi/sec-bert-finetuned-finance-classification" exhibit high accuracies, they do not consistently outperform in all training metrics, suggesting that accuracy alone might not reliably indicate a model's effectiveness in real-world applications.

Given that high accuracy during training does not invariably predict superior real-world performance, our selection of optimal models was guided by additional metrics, including the AUC scores. This approach ensures the final model selection performs well in different contexts and aligns with practical needs.

\begin{figure}[!h]
    \centering
    \includegraphics[width=.85\linewidth]{pictures/number_params_vs_accuracy.png}
    \caption{Number of parameters vs. Accuracy, clustered by adapter configuration.}
    \label{fig:param_acc}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=.85\linewidth]{pictures/training_time_vs_accuracy.png}
    \caption{Training time vs. Accuracy,  clustered by adapter configuration.}
    \label{fig:time_acc}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=.85\linewidth]{pictures/accuracy_lora.png}
    \caption{All models and their corresponding accuracy on LoRA adapter configuration.}
    \label{fig:models_lora_acc}
\end{figure}

\section{Conclusion}
The results show that neither the analysts nor the XGBoost model, which is based on the hard facts in the report, can beat randomness. They only reproduce the current distribution -- i.e. they follow the market trend. However, we have also found that the reports themselves contain valuable information that can be extracted. But to do so, we need LLMs that are fine-tuned to large amounts of financial data. By using Hugging Face's easily accessible models, we have shown that we can extract more value from the reports than the experts themselves.


\bibliographystyle{IEEEtran}
\bibliography{report}

\end{document}
